import json
import os
import sys
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter


# å¼ºåˆ¶åˆ·æ–°è¾“å‡ºï¼Œé˜²æ­¢çœ‹ä¸åˆ°æ‰“å°ä¿¡æ¯
def v_print(msg):
    print(msg, flush=True)


# 1. è·¯å¾„è®¾ç½®
data_folder = r"D:\workerspace\control_qa\data"
json_file = os.path.join(data_folder, "final_control_data_5050.json")
textbook_file = os.path.join(data_folder, "textbook.txt")
db_save_path = r"D:\workerspace\control_qa\vector_db"

documents = []

# --- 2. å¤„ç† JSON é—®ç­”å¯¹ ---
v_print("ğŸ” æ­¥éª¤ 1: å¼€å§‹å¤„ç† JSON é—®ç­”å¯¹...")
if not os.path.exists(json_file):
    v_print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° JSON æ–‡ä»¶: {json_file}")
else:
    with open(json_file, 'r', encoding='utf-8') as f:
        try:
            data = json.load(f)
            v_print(f"ğŸ“– æˆåŠŸè¯»å– JSONï¼ŒåŒ…å« {len(data)} æ¡åŸå§‹æ•°æ®")
            for item in data:
                q = item.get('question', '')
                a = item.get('answer', '')
                if q and a:
                    content = f"é—®é¢˜: {q}\nå›ç­”: {a}"
                    doc = Document(page_content=content, metadata={"source": "ç²¾é€‰é—®ç­”åº“"})
                    documents.append(doc)
        except Exception as e:
            v_print(f"âŒ JSON è§£æå¤±è´¥: {e}")

# --- 3. å¤„ç†æ•™ææ–‡æœ¬ ---
v_print("ğŸ” æ­¥éª¤ 2: å¼€å§‹å¤„ç†æ•™æåŸæ–‡...")
if os.path.exists(textbook_file):
    text = ""
    try:
        with open(textbook_file, 'r', encoding='utf-8') as f:
            text = f.read()
    except:
        with open(textbook_file, 'r', encoding='gbk', errors='ignore') as f:
            text = f.read()

    if text:
        v_print(f"ğŸ“– æˆåŠŸè¯»å–æ•™æï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
        text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
        chunks = text_splitter.split_text(text)
        for i, chunk in enumerate(chunks):
            doc = Document(page_content=chunk, metadata={"source": f"æ•™æåŸæ–‡-ç¬¬{i}æ®µ"})
            documents.append(doc)
    else:
        v_print("âš ï¸ è­¦å‘Š: æ•™æå†…å®¹ä¸ºç©ºï¼")

# --- 4. å‘é‡åŒ–ä¸ä¿å­˜ ---
v_print(f"ğŸ“Š æ­¥éª¤ 3: æ£€æŸ¥è£…è½½æƒ…å†µ... å½“å‰å…±æœ‰ {len(documents)} ä¸ªçŸ¥è¯†ç‰‡æ®µ")

if len(documents) == 0:
    v_print("âŒ ä¸¥é‡é”™è¯¯: æ²¡æœ‰åŠ è½½åˆ°ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œåœæ­¢æ„å»ºæ•°æ®åº“ï¼")
    sys.exit()

v_print("ğŸš€ æ­£åœ¨åŠ è½½ Embedding æ¨¡å‹ (é¦–æ¬¡è¿è¡Œå¯èƒ½è¾ƒæ…¢)...")
# ä½¿ç”¨ RTX 5070 çš„ç®—åŠ›åŠ é€Ÿ
embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-small-zh-v1.5",
    model_kwargs={'device': 'cuda'}
)

v_print("ğŸ’ æ­£åœ¨ç”Ÿæˆå‘é‡ç´¢å¼•å¹¶è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ...")
vector_db = FAISS.from_documents(documents, embeddings)

v_print(f"ğŸ’¾ æ­£åœ¨ä¿å­˜æ•°æ®åº“åˆ°: {db_save_path}")
vector_db.save_local(db_save_path)

v_print("ğŸ‰ ğŸ‰ ğŸ‰ æ­å–œï¼æ•°æ®åº“æ„å»ºæˆåŠŸï¼")