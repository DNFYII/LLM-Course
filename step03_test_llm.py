import os

# --- å…³é”®ä¿®æ”¹ 1: å±è”½æ˜¾å¡ï¼Œå¼ºè¡Œä½¿ç”¨ CPU ---
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


def load_and_chat():
    print("ğŸš€ æ­£åœ¨å‡†å¤‡ä¸‹è½½/åŠ è½½ Qwen2.5-1.5B æ¨¡å‹...")

    model_name = "Qwen/Qwen2.5-1.5B-Instruct"

    # 1. ä¸‹è½½æ¨¡å‹
    model_dir = snapshot_download(model_name)

    # 2. åŠ è½½ Tokenizer
    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)

    # 3. åŠ è½½æ¨¡å‹ (å¼ºåˆ¶ä½¿ç”¨ CPU)
    print("âš ï¸ æ£€æµ‹åˆ° RTX 50ç³»æ˜¾å¡å…¼å®¹æ€§é—®é¢˜ï¼Œæ­£åœ¨åˆ‡æ¢è‡³ CPU æ¨¡å¼è¿è¡Œ...")
    print("ğŸ§  æ­£åœ¨åŠ è½½æ¨¡å‹ (è¿™å¯èƒ½éœ€è¦åå‡ ç§’)...")

    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map="cpu",  # --- å…³é”®ä¿®æ”¹ 2: æŒ‡å®šè·‘åœ¨ CPU ä¸Š ---
        torch_dtype="auto",  # CPU è‡ªåŠ¨é€‰æ‹©ç²¾åº¦
        trust_remote_code=True
    )

    print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼å‡†å¤‡å¯¹è¯...")

    # --- æµ‹è¯•å¯¹è¯ ---
    prompt = "ä½ æ˜¯è°ï¼Ÿè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä¸€ä¸‹ä»€ä¹ˆæ˜¯'è‡ªåŠ¨æ§åˆ¶'ã€‚"
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæ¥è‡ªå—äº¬èˆªç©ºèˆªå¤©å¤§å­¦(NUAA)èˆªå¤©å­¦é™¢çš„ä¸“ä¸šè¯¾ç¨‹åŠ©æ•™ï¼Œä½ çš„åå­—å«'å—èˆªå°æ™º'ã€‚è¯·ç”¨ä¸“ä¸šã€äº²åˆ‡çš„è¯­æ°”å›ç­”å­¦ç”Ÿçš„é—®é¢˜ã€‚"},
        {"role": "user", "content": prompt}
    ]

    # å¤„ç†è¾“å…¥
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    # --- å…³é”®ä¿®æ”¹ 3: è¾“å…¥æ•°æ®ä¹Ÿè¦æ”¾åœ¨ CPU ä¸Š ---
    model_inputs = tokenizer([text], return_tensors="pt").to("cpu")

    # ç”Ÿæˆå›ç­”
    print(f"\nUser: {prompt}")
    print("AI (Qwen) æ­£åœ¨æ€è€ƒ... (CPU å¯èƒ½ä¼šæ…¢ä¸€ç‚¹ç‚¹ï¼Œè¯·è€å¿ƒç­‰å¾…)\n")

    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512
    )

    # è§£ç è¾“å‡º
    generated_ids = [
        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)
    ]
    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    print(f"Qwen: {response}")


if __name__ == "__main__":
    load_and_chat()