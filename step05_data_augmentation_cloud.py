import json
import os
import re
import time
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. ç¯å¢ƒä¸è·¯å¾„é…ç½® ---
# å¿…é¡»å…ˆåœ¨ Kaggle å³ä¾§ Add Input æŒ‚è½½è¿™ä¸¤ä¸ªæ•°æ®é›† [cite: 2025-12-24]
DATA_PATH = "/kaggle/input/nuaa-control-qa/control_knowledge_base"
# MODEL_PATH éœ€æŒ‡å‘ä½ æŒ‚è½½çš„æ¨¡å‹æƒé‡æ•°æ®é›†ä¸­çš„å…·ä½“ç›®å½• [cite: 2025-12-24]
MODEL_PATH = "/kaggle/input/qwen25-15b-weights/qwen_files/Qwen/Qwen2.5-1.5B-Instruct"
OUTPUT_PATH = "/kaggle/working/synthetic_data_5k.json"
DEVICE = "cuda"

print(f"ğŸš€ ç”Ÿäº§ç¯å¢ƒå°±ç»ªã€‚æ¨¡å‹è·¯å¾„: {MODEL_PATH}")

# --- 2. åŠ è½½æœ¬åœ°èµ„æº (0ç§’é¢„çƒ­æ¨¡å¼) ---
print("ğŸ§  æ­£åœ¨ä»æŒ‚è½½ç¡¬ç›˜åŠ è½½çŸ¥è¯†åº“ä¸æ¨¡å‹...")
# ä½¿ç”¨æœ¬åœ° Embedding æ¨¡å‹ [cite: 2025-11-12]
embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")
vector_db = FAISS.load_local(DATA_PATH, embeddings, allow_dangerous_deserialization=True)

# æå–æ‰€æœ‰åŸå§‹æ–‡æœ¬ç‰‡æ®µ [cite: 2025-12-24]
all_docs = [vector_db.docstore.search(vector_db.index_to_docstore_id[i]).page_content
            for i in range(len(vector_db.index_to_docstore_id))]

# ç¦»çº¿åŠ è½½ Qwen2.5-1.5B [cite: 2025-12-24]
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_PATH,
    device_map="auto",
    torch_dtype="auto"
)


# --- 3. 5å€äº§å‡ºå‡ºé¢˜å‡½æ•° (è¾¾æ ‡æ ¸å¿ƒ) ---
def generate_qa_pair(context):
    # å¼ºåˆ¶è¦æ±‚ LaTeX æ ¼å¼ä»¥é€‚é… Obsidian [cite: 2025-10-24, 2025-12-17]
    prompt = f"""ä½ ç°åœ¨æ˜¯å—èˆªè‡ªåŠ¨åŒ–å­¦é™¢æ•™æˆã€‚è¯·æ ¹æ®æ•™æç‰‡æ®µäº§å‡º 5 é“ä¸åŒç±»å‹çš„è€ƒé¢˜ä»¥æ‰©å……é¢˜åº“ï¼š
1. å•é€‰é¢˜ A (å«é€‰é¡¹åŠç­”æ¡ˆ)
2. å•é€‰é¢˜ B (å«é€‰é¡¹åŠç­”æ¡ˆ)
3. å¡«ç©ºé¢˜ (å«ç­”æ¡ˆ)
4. ç®€ç­”é¢˜ (å«ç­”æ¡ˆ)
5. è®¡ç®—åˆ†æé¢˜ (éœ€åŒ…å«å¤æ‚çš„ LaTeX æ¨å¯¼è¿‡ç¨‹åŠç­”æ¡ˆ)

è¦æ±‚ï¼š
- æ‰€æœ‰çš„æ•°å­¦å…¬å¼ï¼ˆå¦‚ $G(s)$, $\omega_n$, ä¼ é€’å‡½æ•°ç­‰ï¼‰å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼å°è£… [cite: 2025-10-24]ã€‚
- å¤æ‚çš„å…¬å¼è¯·ä½¿ç”¨ $$...$$ï¼Œè¡Œå†…å˜é‡ä½¿ç”¨ $...$ [cite: 2025-12-17]ã€‚
- ç¡®ä¿è¾“å‡ºåœ¨ Obsidian ä¸­æ¸…æ™°æ˜äº† [cite: 2025-12-17]ã€‚

æ•™æç‰‡æ®µï¼š
{context}

è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¾“å‡ºï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šå†…å®¹ï¼š
{{
  "choice_1": {{"q": "", "options": ["A.","B.","C.","D."], "a": ""}},
  "choice_2": {{"q": "", "options": ["A.","B.","C.","D."], "a": ""}},
  "fill": {{"q": "", "a": ""}},
  "short": {{"q": "", "a": ""}},
  "calc": {{"q": "", "a": ""}}
}}
"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    # é€‚å½“å¢åŠ  tokens é™åˆ¶ä»¥å®¹çº³ 5 é“é¢˜çš„å†…å®¹ [cite: 2025-12-24]
    generated_ids = model.generate(**model_inputs, max_new_tokens=1536)
    response = tokenizer.batch_decode(generated_ids[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
    return response


# --- 4. é²æ£’æ€§ç”Ÿæˆå¾ªç¯ ---
generated_data = []
test_limit = len(all_docs)

print(f"ğŸ”¥ 5000+ ç›®æ ‡è®¡åˆ’å¯åŠ¨ï¼åŸå§‹ç´ æå…±: {test_limit} æ¡")

for i in tqdm(range(test_limit)):
    try:
        raw_output = generate_qa_pair(all_docs[i])

        # A. ä½¿ç”¨æ­£åˆ™æš´åŠ›æå– JSON å—ï¼Œè§£å†³å¤šä½™æ–‡å­—å¹²æ‰° [cite: 2025-12-24]
        json_match = re.search(r'\{.*\}', raw_output, re.DOTALL)
        if not json_match:
            continue
        clean_json = json_match.group()

        # B. æ ¸å¿ƒä¿®å¤ï¼šé’ˆå¯¹ LaTeX åæ–œæ è¿›è¡Œ JSON è½¬ä¹‰å¤„ç† [cite: 2025-12-24]
        # é€»è¾‘ï¼šå°†å•åæ–œæ æ›¿æ¢ä¸ºåŒåæ–œæ ï¼Œé˜²æ­¢ json.loads å´©æºƒ [cite: 2025-12-24]
        clean_json = clean_json.replace('\\', '\\\\').replace('\\\\\\\\', '\\\\')

        # C. è§£æå¹¶å­˜å‚¨
        data_item = json.loads(clean_json)
        generated_data.append(data_item)

        # D. å®æ—¶æ—¥å¿—ç›‘æ§ï¼šè§£å†³ Kaggle UI åˆ·æ–°å»¶è¿Ÿé—®é¢˜ [cite: 2025-12-24]
        if (i + 1) % 10 == 0:
            total_est = len(generated_data) * 5
            print(
                f"ğŸ“Š è¿›åº¦: {i + 1}/{test_limit} | ä¼°ç®—å·²å­˜é¢˜é‡: {total_est} | æˆåŠŸç‡: {(len(generated_data) / (i + 1)) * 100:.1f}%")

    except Exception:
        continue

# --- 5. æœ€ç»ˆä¿å­˜ ---
with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
    json.dump(generated_data, f, ensure_ascii=False, indent=2)

print(f"âœ… å…¨éƒ¨ä»»åŠ¡å®Œæˆï¼")
print(f"ğŸ“Š æœ€ç»ˆæœ‰æ•ˆé¢˜ç›®æ€»æ•°: {len(generated_data) * 5}")  # æ¯æ¡æ•°æ®å« 5 é¢˜ [cite: 2025-12-24]
print(f"ğŸ“‚ ç»“æœæ–‡ä»¶: {OUTPUT_PATH}")