import json
import os
import time
from tqdm import tqdm
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings

# --- 1. è·¯å¾„ä¸è®¾å¤‡è‡ªåŠ¨é€‚é… ---
# è‡ªåŠ¨æ£€æµ‹æ˜¯åœ¨ Kaggle äº‘ç«¯è¿˜æ˜¯æœ¬åœ° PyCharm
if os.path.exists("/kaggle/input"):
    # äº‘ç«¯é…ç½®
    DATA_PATH = "/kaggle/input/nuaa-control-qa/control_knowledge_base"
    OUTPUT_PATH = "/kaggle/working/synthetic_data_5k.json"
    DEVICE = "cuda"
else:
    # æœ¬åœ°é…ç½®
    DATA_PATH = "./data/control_knowledge_base"
    OUTPUT_PATH = "./data/synthetic_data_5k.json"
    DEVICE = "cpu"  # æœ¬åœ°ç¯å¢ƒé©±åŠ¨æœªç¨³ï¼Œå¼ºåˆ¶ä½¿ç”¨ CPU é¿å…æŠ¥é”™

print(f"ğŸš€ å½“å‰è¿è¡Œç¯å¢ƒ: {'Kaggleäº‘ç«¯' if DEVICE == 'cuda' else 'æœ¬åœ°ç”µè„‘'}")
print(f"ğŸ§  æ­£åœ¨å‡†å¤‡å‡ºé¢˜ç³»ç»Ÿ (ä½¿ç”¨è®¾å¤‡: {DEVICE})...")

# --- 2. åŠ è½½èµ„æº ---
# åŠ è½½ Embedding æ¨¡å‹
embeddings = HuggingFaceEmbeddings(model_name="shibing624/text2vec-base-chinese")

# åŠ è½½çŸ¥è¯†åº“
if not os.path.exists(DATA_PATH):
    raise FileNotFoundError(f"æ‰¾ä¸åˆ°çŸ¥è¯†åº“è·¯å¾„: {DATA_PATH}ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶å¤¹æ˜¯å¦å­˜åœ¨ã€‚")

vector_db = FAISS.load_local(DATA_PATH, embeddings, allow_dangerous_deserialization=True)

# æå–æ‰€æœ‰åŸå§‹æ–‡æœ¬ç‰‡æ®µ
all_docs = [vector_db.docstore.search(vector_db.index_to_docstore_id[i]).page_content
            for i in range(len(vector_db.index_to_docstore_id))]

# åŠ è½½ Qwen æ¨¡å‹
model_dir = snapshot_download("Qwen/Qwen2.5-1.5B-Instruct")
tokenizer = AutoTokenizer.from_pretrained(model_dir)
model = AutoModelForCausalLM.from_pretrained(
    model_dir,
    device_map=DEVICE,
    torch_dtype="auto"
)


# --- 3. æ ¸å¿ƒç”Ÿæˆé€»è¾‘ ---
def generate_qa_pair(context):
    # å¼ºåˆ¶è¦æ±‚ LaTeX æ ¼å¼ä»¥é€‚é… Obsidian [cite: 2025-10-24]
    prompt = f"""ä½ ç°åœ¨æ˜¯å—èˆªè‡ªåŠ¨åŒ–å­¦é™¢çš„æ•™æˆã€‚è¯·æ ¹æ®ä»¥ä¸‹æ•™æç‰‡æ®µå‡ºä¸‰é“é¢˜ï¼š
1. ä¸€é“å•é€‰é¢˜ï¼ˆå«é€‰é¡¹å’Œç­”æ¡ˆï¼‰
2. ä¸€é“å¡«ç©ºé¢˜ï¼ˆå«ç­”æ¡ˆï¼‰
3. ä¸€é“ç®€ç­”é¢˜ï¼ˆå«ç­”æ¡ˆï¼‰

è¦æ±‚ï¼š
- æ‰€æœ‰çš„æ•°å­¦å…¬å¼ã€å˜é‡ï¼ˆå¦‚ G(s), s, ä¼ é€’å‡½æ•°ç­‰ï¼‰å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼å°è£… [cite: 2025-10-24]ã€‚
- å¤æ‚çš„å…¬å¼è¯·ä½¿ç”¨ $$...$$ï¼Œè¡Œå†…å˜é‡ä½¿ç”¨ $...$ [cite: 2025-10-24]ã€‚
- ç¡®ä¿è¾“å‡ºåœ¨ Obsidian ä¸­èƒ½æ¸…æ™°æ˜äº†åœ°æ˜¾ç¤º [cite: 2025-12-17]ã€‚

æ•™æç‰‡æ®µï¼š
{context}

è¯·ä¸¥æ ¼æŒ‰ JSON æ ¼å¼è¾“å‡ºï¼š
{{
  "choice_question": "",
  "choice_answer": "",
  "fill_question": "",
  "fill_answer": "",
  "short_question": "",
  "short_answer": ""
}}
"""
    messages = [{"role": "user", "content": prompt}]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(DEVICE)

    generated_ids = model.generate(**model_inputs, max_new_tokens=800)
    response = tokenizer.batch_decode(generated_ids[:, model_inputs.input_ids.shape[1]:], skip_special_tokens=True)[0]
    return response


# --- 4. å¾ªç¯ç”Ÿæˆ ---
generated_data = []
# æœ¬åœ°æµ‹è¯•å»ºè®®åªè·‘ 2 æ¡ï¼ŒKaggle å…¨é‡è·‘è¯·æ”¹ä¸º len(all_docs)
test_limit =len(all_docs)

print(f"ğŸ“ å¼€å§‹å‡ºé¢˜ä»»åŠ¡ï¼Œç›®æ ‡æ¡æ•°: {test_limit}")

for i in tqdm(range(test_limit)):
    try:
        raw_output = generate_qa_pair(all_docs[i])
        # æ¸…ç†å¯èƒ½å­˜åœ¨çš„ Markdown æ ‡è®°
        clean_json = raw_output.replace("```json", "").replace("```", "").strip()
        generated_data.append(json.loads(clean_json))
    except Exception as e:
        print(f"è·³è¿‡ç¬¬ {i} æ¡è®°å½•ï¼ŒåŸå› : {e}")
        continue

# --- 5. ä¿å­˜ç»“æœ ---
with open(OUTPUT_PATH, 'w', encoding='utf-8') as f:
    json.dump(generated_data, f, ensure_ascii=False, indent=2)

print(f"âœ… ä»»åŠ¡å®Œæˆï¼ç»“æœå·²å­˜è‡³: {OUTPUT_PATH}")