import time
import os
import torch
from modelscope import snapshot_download
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- âš™ï¸ é…ç½®åŒºåŸŸ ---
# å¦‚æœæ˜¯åœ¨æœ¬åœ°è·‘ï¼Œä¿æŒ "-1" (ä½¿ç”¨ CPU)
# å¦‚æœæ˜¯åœ¨ Colab/Kaggle è·‘ï¼Œæ”¹æˆ "0" (ä½¿ç”¨ GPU)
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"


def run_benchmark():
    print("ğŸš€ æ­£åœ¨åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹é€Ÿ...")
    model_name = "Qwen/Qwen2.5-1.5B-Instruct"
    model_dir = snapshot_download(model_name)

    # è‡ªåŠ¨åˆ¤æ–­è®¾å¤‡ï¼šå¦‚æœæœ‰ GPU ä¸” CUDA_VISIBLE_DEVICES ä¸æ˜¯ -1ï¼Œå°±ç”¨ GPUï¼Œå¦åˆ™ CPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"ğŸ“Š å½“å‰æµ‹è¯•è®¾å¤‡: {device.upper()}")
    if device == "cpu":
        print("âš ï¸ æ³¨æ„ï¼šCPU é€Ÿåº¦é€šå¸¸è¾ƒæ…¢ï¼Œè¿™æ˜¯æ­£å¸¸çš„ã€‚")

    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        model_dir,
        device_map=device,
        torch_dtype="auto",
        trust_remote_code=True
    )

    # --- ğŸ§ª å¼€å§‹æµ‹è¯• ---
    # è®©å®ƒå†™é•¿ä¸€ç‚¹ï¼Œæµ‹å¾—æ‰å‡†
    prompt = "è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹'è‡ªåŠ¨æ§åˆ¶åŸç†'è¿™é—¨è¯¾ç¨‹çš„ä¸»è¦å†…å®¹ï¼ŒåŒ…æ‹¬ç»å…¸æ§åˆ¶ç†è®ºå’Œç°ä»£æ§åˆ¶ç†è®ºçš„åŒºåˆ«ï¼Œå­—æ•°åœ¨200å­—ä»¥ä¸Šã€‚"
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªåŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt}
    ]

    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to(device)

    print("-" * 30)
    print("â±ï¸ å¼€å§‹ç”Ÿæˆï¼Œè¯·ç¨å€™...")

    # 1. è®°å½•å¼€å§‹æ—¶é—´
    start_time = time.time()

    # 2. ç”Ÿæˆå†…å®¹ (å¼ºåˆ¶ç”Ÿæˆè‡³å°‘ 100 ä¸ª token ä»¥ç¡®ä¿æµ‹è¯•æœ‰æ•ˆ)
    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,  # å…è®¸ç”Ÿæˆçš„æœ€å¤§é•¿åº¦
        min_new_tokens=100  # å¼ºåˆ¶å®ƒå¤šå†™ç‚¹
    )

    # 3. è®°å½•ç»“æŸæ—¶é—´
    end_time = time.time()

    # --- ğŸ§® è®¡ç®—é€Ÿåº¦ ---
    # æå–æ–°ç”Ÿæˆçš„ token (å»æ‰è¾“å…¥çš„ token)
    input_token_len = model_inputs.input_ids.shape[1]
    output_token_len = generated_ids.shape[1]
    new_tokens = output_token_len - input_token_len

    duration = end_time - start_time
    speed = new_tokens / duration

    response = tokenizer.decode(generated_ids[0][input_token_len:], skip_special_tokens=True)

    print("-" * 30)
    print(f"âœ… ç”Ÿæˆå®Œæ¯•ï¼")
    print(f"ğŸ“ ç”Ÿæˆå†…å®¹é¢„è§ˆ: {response[:50]}...")  # åªæ‰“å°å‰50ä¸ªå­—çœ‹çœ‹
    print("-" * 30)
    print(f"ğŸ”¢ ç”Ÿæˆ Token æ•°: {new_tokens}")
    print(f"â±ï¸ è€—æ—¶: {duration:.2f} ç§’")
    print(f"ğŸš€ æ¨ç†é€Ÿåº¦ (Tokens/s): {speed:.2f}")
    print("-" * 30)


if __name__ == "__main__":
    run_benchmark()