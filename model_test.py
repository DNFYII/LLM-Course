import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import os

# 1. ä¿®æ­£åçš„è·¯å¾„ (ä½¿ç”¨ r å‰ç¼€é˜²æ­¢è½¬ä¹‰)
base_model_path = r"D:\workerspace\models\Qwen\Qwen2___5-1___5B-Instruct"
lora_path = r"D:\workerspace\control_qa\model\qwen-lora"

# ã€éªŒè¯æ­¥éª¤ã€‘å…ˆæ£€æŸ¥è·¯å¾„æ˜¯å¦å­˜åœ¨ï¼Œé˜²æ­¢æ¨¡å‹åŠ è½½åˆ°ä¸€åŠæŠ¥é”™
if not os.path.exists(base_model_path):
    print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°åŸºç¡€æ¨¡å‹è·¯å¾„ï¼Œè¯·æ£€æŸ¥ï¼š{base_model_path}")
elif not os.path.exists(lora_path):
    print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ° LoRA æƒé‡è·¯å¾„ï¼Œè¯·æ£€æŸ¥ï¼š{lora_path}")
else:
    print("âœ… è·¯å¾„æ£€æŸ¥é€šè¿‡ï¼Œå‡†å¤‡åŠ è½½æ¨¡å‹è‡³ RTX 5070...")

    # 2. åŠ è½½åˆ†è¯å™¨å’Œæ¨¡å‹
    tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)

    # ä½¿ç”¨ dtype=torch.float16 å‡å°‘æ˜¾å­˜å ç”¨
    model = AutoModelForCausalLM.from_pretrained(
        base_model_path,
        device_map="auto",
        dtype=torch.float16,
        low_cpu_mem_usage=True,
        trust_remote_code=True
    )

    # 3. åŠ è½½å¾®è°ƒè¡¥ä¸ (LoRA)
    model = PeftModel.from_pretrained(model, lora_path)
    print("ğŸš€ æ¨¡å‹ä¸å¾®è°ƒæƒé‡åŠ è½½å®Œæˆï¼")

    # 4. æµ‹è¯•æé—®
    prompt = "ä»€ä¹ˆæ˜¯é—­ç¯æ§åˆ¶ç³»ç»Ÿçš„ç¨³å®šæ€§ï¼Ÿ"
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ§åˆ¶åŸç†ä¸“å®¶ã€‚"},
        {"role": "user", "content": prompt}
    ]
    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
    model_inputs = tokenizer([text], return_tensors="pt").to("cuda")

    generated_ids = model.generate(
        **model_inputs,
        max_new_tokens=512,
        do_sample=True,
        temperature=0.7
    )

    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    print("\n--- ä¸“å®¶å›ç­” ---")
    print(response.split("assistant\n")[-1])