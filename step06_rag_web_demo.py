import streamlit as st
import torch
import os
import re
from transformers import AutoModelForCausalLM, AutoTokenizer, TextIteratorStreamer
from langchain_community.vectorstores import FAISS
from langchain_huggingface import HuggingFaceEmbeddings
from threading import Thread

# ==========================================
# 1. é…ç½®åŒºåŸŸ
# ==========================================
ST_TITLE = "ğŸ¤– ControlExpert 2.0 - è‡ªåŠ¨åŒ–ä¸“ä¸šæ™ºèƒ½ç»ˆç«¯"
# è¯·ç¡®ä¿è·¯å¾„æ­£ç¡®
BASE_MODEL_PATH = r"D:\workerspace\models\Qwen\Qwen2___5-1___5B-Instruct"
DB_PATH = r"D:\workerspace\control_qa\vector_db"


# ==========================================
# 2. æ ¸å¿ƒé€»è¾‘ï¼šèµ„æºåŠ è½½ (å•ä¾‹æ¨¡å¼)
# ==========================================
@st.cache_resource
def load_control_expert_core():
    tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_PATH, trust_remote_code=True)
    model = AutoModelForCausalLM.from_pretrained(
        BASE_MODEL_PATH,
        device_map="auto",
        dtype=torch.float16,
        trust_remote_code=True
    )
    embeddings = HuggingFaceEmbeddings(
        model_name="BAAI/bge-small-zh-v1.5",
        model_kwargs={'device': 'cuda'}
    )
    vector_db = FAISS.load_local(DB_PATH, embeddings, allow_dangerous_deserialization=True)
    return tokenizer, model, vector_db


# ==========================================
# 3. é¡µé¢é…ç½®
# ==========================================
st.set_page_config(page_title=ST_TITLE, page_icon="ğŸ¤–", layout="wide")

# ä¾§è¾¹æ 
with st.sidebar:
    st.header("ğŸ–¥ï¸ ç³»ç»ŸçŠ¶æ€é¢æ¿")
    with st.spinner("æ­£åœ¨å”¤é†’è®¡ç®—æ ¸å¿ƒ..."):
        tokenizer, model, vector_db = load_control_expert_core()

    st.success("âœ… ç³»ç»Ÿå·²å°±ç»ª")
    st.info(f"ğŸ“š çŸ¥è¯†åº“: 5050 QA + æ•™æåŸæ–‡")
    st.info(f"ğŸ§  æ¨¡å‹: Qwen2.5-1.5B-Instruct")

    st.markdown("---")

    # [è°ƒæ•´ 1] å°†"æ¸…é™¤è®°å½•"æŒ‰é’®ä¸Šç§»ï¼Œä½œä¸ºå¸¸ç”¨åŠŸèƒ½
    if st.button("ğŸ”„ æ¸…é™¤æ‰€æœ‰å¯¹è¯è®°å½•", use_container_width=True):
        st.session_state.messages = []
        st.rerun()

    # [è°ƒæ•´ 2] åˆ©ç”¨ç©ºè¡Œåˆ¶é€ â€œè§†è§‰ä¸‹æ²‰â€æ•ˆæœï¼ŒæŠŠè®¾ç½®æŒ¤åˆ°æœ€ä¸‹é¢
    # è¿™é‡Œæ·»åŠ äº†ä¸€äº›ç©ºè¡Œï¼Œè®©ä¸‹é¢çš„å…ƒç´ åœ¨è§†è§‰ä¸Šé è¿‘åº•éƒ¨
    st.markdown("<br>" * 15, unsafe_allow_html=True)

    st.markdown("---")
    # [è°ƒæ•´ 3] å¼€å‘è€…é€‰é¡¹ç°åœ¨ä½äºæœ€åº•éƒ¨ (çº¢æ¡†ä½ç½®)
    with st.expander("âš™ï¸ å¼€å‘è€…é€‰é¡¹ (é«˜çº§è®¾ç½®)", expanded=False):
        st.caption("è°ƒæ•´ RAG æ£€ç´¢çµæ•åº¦ä¸è°ƒè¯•æ¨¡å¼")
        threshold = st.slider("æ£€ç´¢ç›¸å…³åº¦é˜ˆå€¼", 0.0, 1.0, 0.45, 0.05)
        debug_mode = st.toggle("ğŸ› ï¸ å¼€å¯è°ƒè¯•æ¨¡å¼", value=False)

st.title(ST_TITLE)
st.caption("ä¸“æ³¨è‡ªåŠ¨æ§åˆ¶åŸç†ä¸“ä¸šé—®ç­”ã€‚")


# ==========================================
# ğŸ”¥ æ ¸å¿ƒä¿®å¤ï¼šæ™ºèƒ½æ„ŸçŸ¥å‹ LaTeX æ¸²æŸ“å¼•æ“ (æœ€ç»ˆç‰ˆ - ä¿æŒä¸å˜)
# ==========================================
def format_latex(text):
    """
    ControlExpert 2.0 æ¸²æŸ“å¼•æ“ - æ™ºèƒ½æ„ŸçŸ¥ç‰ˆ
    """
    # 1. æ¸…ç† Markdown ä»£ç å—å¹²æ‰°
    text = text.replace("```latex", "").replace("```", "").replace("`", "")

    # 2. ç»Ÿä¸€ LaTeX æ‹¬å·æ ‡å‡†
    text = text.replace(r"\[", "\n$$\n")
    text = text.replace(r"\]", "\n$$\n")
    text = text.replace(r"\(", "$")
    text = text.replace(r"\)", "$")

    # 3. æ¢è¡Œç¬¦å¢å¼º
    text = text.replace(r"\begin", "@@BEGIN@@")
    text = text.replace(r"\end", "@@END@@")
    text = text.replace(r"\hline", "@@HLINE@@")
    text = text.replace(r"\frac", "@@FRAC@@")

    text = re.sub(r"\\{2,}", r"\\\\\\\\", text)
    text = re.sub(r"([^\\])\\\s+s", r"\1\\\\\\\\ s", text)

    text = text.replace("@@BEGIN@@", r"\begin")
    text = text.replace("@@END@@", r"\end")
    text = text.replace("@@HLINE@@", r"\hline")
    text = text.replace("@@FRAC@@", r"\frac")

    # 4. ã€æ™ºèƒ½å°è£…ã€‘ Smart Wrapping
    def smart_wrap_start(match):
        prefix = match.group(1) or ""
        content = match.group(2)
        if "$$" in prefix:
            return match.group(0)
        else:
            return f"\n$$\n{content}"

    text = re.sub(r"(\$\$\s*)?(\\begin\{.*?\})", smart_wrap_start, text, flags=re.IGNORECASE)

    def smart_wrap_end(match):
        content = match.group(1)
        suffix = match.group(2) or ""
        if "$$" in suffix:
            return match.group(0)
        else:
            return f"{content}\n$$\n"

    text = re.sub(r"(\\end\{.*?\})(\s*\$\$)?", smart_wrap_end, text, flags=re.IGNORECASE)

    # 5. æœ€ç»ˆæ¸…æ´—
    text = re.sub(r"\$\$\s*\$\$", "", text)
    text = re.sub(r"\n{3,}", "\n\n", text)

    return text


# ==========================================
# 4. å¯¹è¯é€»è¾‘
# ==========================================
if "messages" not in st.session_state:
    st.session_state.messages = []

# æ¸²æŸ“å†å²æ¶ˆæ¯
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# è¾“å…¥æ¡†
if prompt := st.chat_input("è¯·è¾“å…¥è‡ªæ§åŸç†é—®é¢˜..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    with st.spinner("æ­£åœ¨æ£€ç´¢æ•™æåº“..."):
        # è·å– Top-3 ç»“æœ
        results = vector_db.similarity_search_with_relevance_scores(prompt, k=3)
        # æ ¹æ®é˜ˆå€¼è¿‡æ»¤
        valid_docs = [doc for doc, score in results if score > threshold]

    with st.chat_message("assistant"):
        # é€»è¾‘ä¼˜åŒ–ï¼šå¦‚æœæ²¡æœ‰æ£€ç´¢åˆ°å†…å®¹ï¼Œç»™ç”¨æˆ·æ›´æ˜ç¡®çš„æç¤º
        if not valid_docs:
            response = f"âš ï¸ **æ£€ç´¢æœªå‘½ä¸­**\n\nå½“å‰ç›¸å…³åº¦é˜ˆå€¼è®¾ä¸º `{threshold}`ï¼Œæœªåœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°è¶³å¤Ÿç›¸å…³çš„çŸ¥è¯†ç‚¹ã€‚\n\nğŸ’¡ **å»ºè®®**ï¼š\n1. å°è¯•åœ¨å·¦ä¾§â€œå¼€å‘è€…é€‰é¡¹â€ä¸­é™ä½é˜ˆå€¼ (ä¾‹å¦‚è°ƒè‡³ 0.3)ã€‚\n2. æ¢ä¸€ç§æ›´å‡†ç¡®çš„æé—®æ–¹å¼ã€‚"
            st.warning(response)
            st.session_state.messages.append({"role": "assistant", "content": response})
        else:
            context = "\n".join([doc.page_content for doc in valid_docs])
            # å»é‡æ¥æº
            sources = " | ".join(list(set([doc.metadata.get("source", "æœªçŸ¥æ¥æº") for doc in valid_docs])))

            # ğŸš€ æç¤ºè¯å¢å¼º
            system_prompt = r"""ä½ æ˜¯ä¸€ä¸ªè‡ªåŠ¨æ§åˆ¶åŸç†ä¸“å®¶ã€‚
1. æ‰€æœ‰æ•°å­¦å…¬å¼å¿…é¡»ä½¿ç”¨ LaTeX æ ¼å¼ã€‚
2. ã€åŠ³æ–¯è¡¨/çŸ©é˜µä¸“ç”¨è§„åˆ™ã€‘
   - å¿…é¡»ä½¿ç”¨ `\begin{array}` ç¯å¢ƒæ„å»ºã€‚
   - **ä¸¥ç¦**ä½¿ç”¨ Markdown è¡¨æ ¼ (|---|)ã€‚
   - **ä¸¥ç¦**ä½¿ç”¨ Markdown ä»£ç å— (```)ã€‚
   - æ¯ä¸€è¡Œç»“æŸæ—¶ï¼Œè¯·ä¸¥æ ¼è¾“å‡º `\\` (åŒåæ–œæ ) è¡¨ç¤ºæ¢è¡Œã€‚
   - **ç»å¯¹ä¸è¦**è¾“å‡º `\\\` (ä¸‰æ–œæ ) æˆ– `\ \` (å•æ–œæ ç©ºæ ¼)ã€‚
3. é‡åˆ°çŸ©é˜µæˆ–å¤æ‚ç®—å¼ï¼Œè¯·ç›´æ¥è¾“å‡ºå…¬å¼å—ï¼Œä¸è¦åŠ å¤šä½™çš„è§£é‡Šæ–‡æœ¬ã€‚
4. **æ­£ç¡®ç¤ºä¾‹ï¼š**
   $$
   \begin{array}{c|cc}
   s^2 & 1 & 2 \\
   s^1 & 3 & 4 \\
   s^0 & 5 & 0
   \end{array}
   $$"""

            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": f"å‚è€ƒèµ„æ–™ï¼š\n{context}\n\né—®é¢˜ï¼š{prompt}"}
            ]

            input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True,
                                                      return_tensors="pt").to(model.device)
            streamer = TextIteratorStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)

            gen_kwargs = dict(input_ids=input_ids, streamer=streamer, max_new_tokens=1024, temperature=0.3)
            Thread(target=model.generate, kwargs=gen_kwargs).start()

            response_placeholder = st.empty()
            full_response = ""
            for new_text in streamer:
                full_response += new_text
                # å®æ—¶åº”ç”¨æ¸…æ´—é€»è¾‘
                display_text = format_latex(full_response)
                response_placeholder.markdown(display_text + "â–Œ")

            # æœ€ç»ˆæ˜¾ç¤º
            final_formatted = format_latex(full_response)
            final_display = final_formatted + f"\n\n--- \n ğŸ“š **å‚è€ƒæ¥æº**: {sources}"
            response_placeholder.markdown(final_display)

            # ğŸ”¥ è°ƒè¯•ä¿¡æ¯å±•ç¤ºåŒº (é›†æˆåœ¨ä»£ç ä¸­ï¼Œé»˜è®¤å…³é—­ï¼Œå¼€å…³åœ¨ä¾§è¾¹æ )
            if debug_mode:
                with st.expander("ğŸ› ï¸ å·¥ç¨‹å¸ˆè°ƒè¯•è§†å›¾ (Raw Data)", expanded=True):
                    col1, col2 = st.columns(2)
                    with col1:
                        st.caption("1. æ¨¡å‹åŸå§‹è¾“å‡º (Raw)")
                        st.code(full_response, language="latex")
                    with col2:
                        st.caption("2. æ¸…æ´—åæ•°æ® (Formatted)")
                        st.code(final_formatted, language="latex")
                    if "$$" not in final_formatted:
                        st.error("ğŸš¨ è­¦å‘Šï¼šæ¸…æ´—åçš„æ•°æ®ä¸­æœªæ£€æµ‹åˆ° $$ ç¬¦å·ï¼Œæ¸²æŸ“å¿…å°†å¤±è´¥ï¼")
                    else:
                        st.success("âœ… æ£€æµ‹åˆ° $$ ç¬¦å·ï¼ŒMathJax åº”è¯¥å·²æ¿€æ´»ã€‚")

            st.session_state.messages.append({"role": "assistant", "content": final_display})